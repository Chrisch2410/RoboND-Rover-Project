{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rover Project Test Notebook\n",
    "\n",
    "### <span style=\"color:blue\"> Table of Content: </span>\n",
    "\n",
    "<a href='#1'>1. Project Notes </a>\n",
    "\n",
    "<a href='#2'>2. Import required Python Libraries </a>\n",
    "\n",
    "<a href='#3'>3. Prespective Transformation </a>\n",
    "\n",
    "<a href='#4'>4. Color Thresholding </a>\n",
    "\n",
    "<a href='#5'>5. Thresholding of Warped Images </a>\n",
    "\n",
    "<a href='#6'>6. Coordinates Transformation </a>\n",
    "\n",
    "<a href='#7'>7. Create Data Container Class </a>\n",
    "\n",
    "<a href='#8'>8. Process_Image Function </a>\n",
    "\n",
    "<a href='#9'>9. Make a video from processed image data </a>\n",
    "\n",
    "<a href='#10'>10. Play Video </a>\n",
    "\n",
    "\n",
    "## 1.  <span style=\"color:blue\"> Project Notes: </span><a id='1'></a>\n",
    "\n",
    "* Simulator Selected Screen Resolution: 1024 x 640\n",
    "* Simulator Graphics Quality: Good\n",
    "\n",
    "## 2. <span style=\"color:blue\"> Import required Python Libraries </span><a id='2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style> code {background-color : orange !important;} </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# required imports\n",
    "import numpy as np                       # For array maths\n",
    "import matplotlib.pyplot as plt          # For ploting results\n",
    "import matplotlib.image as mpimg         # For images reading\n",
    "import cv2                               # OpenCV for perspective transform\n",
    "import glob                              # For reading in a list of images from a folder\n",
    "import pandas as pd                      # For reading csv file as a dataframe\n",
    "import imageio                           # For reading image and video.\n",
    "imageio.plugins.ffmpeg.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <span style=\"color:blue\"> Prespective Transformation </span><a id='3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Calibration Data and show sample\n",
    "\n",
    "example_grid = '../calibration_images/example_grid1.jpg'\n",
    "example_rock = '../calibration_images/example_rock1.jpg'\n",
    "\n",
    "grid_img = mpimg.imread(example_grid)       # get grid example\n",
    "rock_img = mpimg.imread(example_rock)       # get rock example\n",
    "\n",
    "\n",
    "# define prespecive transformation function\n",
    "\n",
    "def perspect_transform(img, src, dst):\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src, dst)  # Get transform matrix using src and dst boxes\n",
    "    warped = cv2.warpPerspective(img,          # Warp image keep same size as input image \n",
    "                                 M, \n",
    "                                 (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    \n",
    "    return warped\n",
    "\n",
    "# Defined to warp the image to a grid where each 10x10 pixel square represents 1 square meter\n",
    "\n",
    "hs_size = 10/2             # half the size of one square\n",
    "b_ofst  = 6                # bottom offset to account for distance from rover edge to 1st camera visiable point\n",
    "x_cntr  = grid_img.shape[1]/2 # center of the image x axis\n",
    "y_end   = grid_img.shape[0]   # end of y or bottom of image\n",
    "\n",
    "src = np.float32([\n",
    "                 [14, 140],      # Left Bottom\n",
    "                 [301 ,140],     # Right Bottom\n",
    "                 [200, 96],      # Right Top\n",
    "                 [118, 96]       # Left Top\n",
    "                 ])              # above data captured from grid calibration image.\n",
    "\n",
    "dst = np.float32([\n",
    "                 [x_cntr - hs_size, y_end - b_ofst],             # Left Bottom\n",
    "                 [x_cntr + hs_size, y_end - b_ofst],             # Right Bottom\n",
    "                 [x_cntr + hs_size, y_end - 2*hs_size - b_ofst], # Right Top\n",
    "                 [x_cntr - hs_size, y_end - 2*hs_size - b_ofst], # Left Top\n",
    "                 ])\n",
    "\n",
    "# warp images using defined src and dst boxes\n",
    "\n",
    "navi_w = perspect_transform(grid_img, src, dst) #warp the navi/obst image\n",
    "rock_w = perspect_transform(rock_img, src, dst) #warp the rock image\n",
    "\n",
    "\n",
    "# Display images\n",
    "f, ax = plt.subplots(2, 2, figsize=(24,12))\n",
    "f.suptitle('Prespective Transformation', fontsize=28)\n",
    "\n",
    "ax[0,0].imshow(grid_img)\n",
    "ax[0,0].set_title('Grid Sample', fontsize=24)\n",
    "\n",
    "ax[1,0].imshow(rock_img)\n",
    "ax[1,0].set_title('Rock Sample', fontsize=24)\n",
    "\n",
    "ax[0,1].imshow(navi_w)\n",
    "ax[0,1].set_title('Grid Warped', fontsize=24)\n",
    "\n",
    "ax[1,1].imshow(rock_w)\n",
    "ax[1,1].set_title('Rock Warped', fontsize=24)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Save figure for project report\n",
    "f.savefig('../output/warp_fun.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <span style=\"color:blue\"> Color Thresholding </span><a id='4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all images from test data set folder\n",
    "path = '../test_dataset/IMG/*'                # path to recorded images\n",
    "img_list = glob.glob(path)                    # read all images to a glob object\n",
    "\n",
    "idx = np.random.randint(0, len(img_list)-1)   # Pick on random image\n",
    "sample_img = mpimg.imread(img_list[idx])\n",
    "\n",
    "\n",
    "# Define a function to threshold warped image and show navigable/obstacles areas\n",
    "def navi_thresh(img):\n",
    "\n",
    "    # Threshold of RGB > 160 does a nice job of identifying ground pixels only\n",
    "    rgb_thr=(160,160,160)\n",
    "     \n",
    "    # mask will contain a boolean array with \"True\" for each pixel above threshold\n",
    "    mask = (img[:,:,0] > rgb_thr[0]) \\\n",
    "         & (img[:,:,1] > rgb_thr[1]) \\\n",
    "         & (img[:,:,2] > rgb_thr[2])\n",
    "\n",
    "    # Create an array of zeros same xy size as img, but single channel\n",
    "    # Index the array with the mask and set to 1\n",
    "    navi = np.zeros_like(img[:,:,0])\n",
    "    navi[mask] = 1\n",
    "    \n",
    "    navi[:int(img.shape[0]*.5),:]=0 # clip upper 50% of image to improve fidelity\n",
    "    \n",
    "    # Create an array of ones same xy size as img, but single channel\n",
    "    # Index the array with the mask and set to 0\n",
    "    obst = np.ones_like(img[:,:,0])\n",
    "    obst[mask] = 0\n",
    "    \n",
    "    obst[:int(img.shape[0]*.5),:]=1 # clip upper 50% of image to improve fidelity\n",
    "    \n",
    "    return navi,obst                        # Return both images\n",
    "\n",
    "\n",
    "# Define a function to threshold rock calibration image and isolate the rock.\n",
    "def rock_thresh(img):\n",
    "    \n",
    "    hsv_img=cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower = np.array([0,200,100])\n",
    "    upper = np.array([179,255,255])\n",
    "    \n",
    "    rock_t = cv2.inRange(hsv_img, lower, upper)\n",
    "      \n",
    "    return rock_t\n",
    "\n",
    "\n",
    "# test thresholding functions on sample images\n",
    "navi_t,obst_t = navi_thresh(sample_img)  # Threshold warped image to show both navigable and obstcales areas\n",
    "rock_t        = rock_thresh(rock_img)           # Threshold calibration image to isolate the rock\n",
    "\n",
    "# Display the three threshold images\n",
    "\n",
    "f, ax = plt.subplots(3, 2, figsize=(20,15))\n",
    "f.suptitle('Color Thresholding', fontsize=28)\n",
    "\n",
    "ax[0,0].imshow(sample_img)\n",
    "ax[0,0].set_title('Sample Image', fontsize=24)\n",
    "\n",
    "ax[0,1].imshow(navi_t,cmap='gray')\n",
    "ax[0,1].set_title('Navigable Area Threshold', fontsize=24)\n",
    "\n",
    "ax[1,0].imshow(sample_img)\n",
    "ax[1,0].set_title('Sample Image', fontsize=24)\n",
    "\n",
    "ax[1,1].imshow(obst_t,cmap='gray')\n",
    "ax[1,1].set_title('Obstacles Area Threshold', fontsize=24)\n",
    "\n",
    "ax[2,0].imshow(rock_img)\n",
    "ax[2,0].set_title('Rock Image', fontsize=24)\n",
    "\n",
    "ax[2,1].imshow(rock_t,cmap='gray')\n",
    "ax[2,1].set_title('Rock Threshold', fontsize=24)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure for project report\n",
    "f.savefig('../output/thresh_fun.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <span style=\"color:blue\"> Thresholding of Warped Images </span><a id='5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test thresholding functions on wraped images\n",
    "\n",
    "navi_wt,obst_wt = navi_thresh(navi_w)  # Threshold warped image to show both navigable and obstcales areas\n",
    "rock_wt         = rock_thresh(rock_w)  # Threshold calibration image to isolate the rock\n",
    "\n",
    "# Display the three threshold images\n",
    "\n",
    "f, ax = plt.subplots(3, 2, figsize=(20,15))\n",
    "f.suptitle('Thresholding of Warped Images', fontsize=28)\n",
    "\n",
    "ax[0,0].imshow(sample_img)\n",
    "ax[0,0].set_title('Sample Image', fontsize=24)\n",
    "\n",
    "ax[0,1].imshow(navi_wt,cmap='gray')\n",
    "ax[0,1].set_title('Navigable Area Thres/Warp', fontsize=24)\n",
    "\n",
    "ax[1,0].imshow(sample_img)\n",
    "ax[1,0].set_title('Sample Image', fontsize=24)\n",
    "\n",
    "ax[1,1].imshow(obst_wt,cmap='gray')\n",
    "ax[1,1].set_title('Obstacles Area Thres/Warp', fontsize=24)\n",
    "\n",
    "ax[2,0].imshow(rock_img)\n",
    "ax[2,0].set_title('Rock Image', fontsize=24)\n",
    "\n",
    "ax[2,1].imshow(rock_wt,cmap='gray')\n",
    "ax[2,1].set_title('Rock Thres/Warp', fontsize=24)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Save figure for project report\n",
    "f.savefig('../output/threshwarp_fun.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <span style=\"color:blue\"> Coordinates Transformation </span><a id='6'></a>\n",
    "### 6.1 Coordinate Transformations Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# Define a function to convert from image coords to rover coords\n",
    "def rover_coords(binary_img):\n",
    "    # Identify nonzero pixels\n",
    "    ypos, xpos = binary_img.nonzero()\n",
    "    # Calculate pixel positions with reference to the rover position being at the \n",
    "    # center bottom of the image.  \n",
    "    x_pixel = -(ypos - binary_img.shape[0]).astype(np.float)\n",
    "    y_pixel = -(xpos - binary_img.shape[1]/2 ).astype(np.float)\n",
    "    return x_pixel, y_pixel\n",
    "\n",
    "# Define a function to convert to radial coords in rover space\n",
    "def to_polar_coords(x_pixel, y_pixel):\n",
    "    # Convert (x_pixel, y_pixel) to (distance, angle) \n",
    "    # in polar coordinates in rover space\n",
    "    # Calculate distance to each pixel\n",
    "    dist = np.sqrt(x_pixel**2 + y_pixel**2)\n",
    "    # Calculate angle away from vertical for each pixel\n",
    "    angles = np.arctan2(y_pixel, x_pixel)\n",
    "    return dist, angles\n",
    "\n",
    "# Define a function to map rover space pixels to world space\n",
    "def rotate_pix(xpix, ypix, yaw):\n",
    "    # Convert yaw to radians\n",
    "    yaw_rad = yaw * np.pi / 180\n",
    "    xpix_rotated = (xpix * np.cos(yaw_rad)) - (ypix * np.sin(yaw_rad))\n",
    "                            \n",
    "    ypix_rotated = (xpix * np.sin(yaw_rad)) + (ypix * np.cos(yaw_rad))\n",
    "    # Return the result  \n",
    "    return xpix_rotated, ypix_rotated\n",
    "\n",
    "def translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale): \n",
    "    # Apply a scaling and a translation\n",
    "    xpix_translated = (xpix_rot / scale) + xpos\n",
    "    ypix_translated = (ypix_rot / scale) + ypos\n",
    "    # Return the result  \n",
    "    return xpix_translated, ypix_translated\n",
    "\n",
    "\n",
    "# Define a function to apply rotation and translation (and clipping)\n",
    "# Once you define the two functions above this function should work\n",
    "def pix_to_world(xpix, ypix, xpos, ypos, yaw, world_size, scale):\n",
    "    # Apply rotation\n",
    "    xpix_rot, ypix_rot = rotate_pix(xpix, ypix, yaw)\n",
    "    # Apply translation\n",
    "    xpix_tran, ypix_tran = translate_pix(xpix_rot, ypix_rot, xpos, ypos, scale)\n",
    "    # Perform rotation, translation and clipping all at once\n",
    "    x_pix_world = np.clip(np.int_(xpix_tran), 0, world_size - 1)\n",
    "    y_pix_world = np.clip(np.int_(ypix_tran), 0, world_size - 1)\n",
    "    # Return the result\n",
    "    return x_pix_world, y_pix_world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Test Transformations Functions: rover_coords() and to_polar_coords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "outputExpanded": false
   },
   "outputs": [],
   "source": [
    "# Grab another random image\n",
    "idx = np.random.randint(0, len(img_list)-1)\n",
    "navi_img = mpimg.imread(img_list[idx])\n",
    "\n",
    "# Transform images\n",
    "navi_w = perspect_transform(navi_img, src, dst)\n",
    "navi_t,obst_t = navi_thresh(navi_w)\n",
    "\n",
    "# Calculate pixel values in rover-centric coords and distance/angle to all pixels\n",
    "xpix, ypix = rover_coords(navi_t)\n",
    "dist, angles = to_polar_coords(xpix, ypix)\n",
    "mean_dir = np.mean(angles)\n",
    "angle = np.clip(mean_dir*180/np.pi,-15,15)\n",
    "\n",
    "print(\"dist =\",dist[0:3])\n",
    "print(\"angles =\",angles[0:3])\n",
    "print(\"mean_dir =\",mean_dir)\n",
    "print(\"Angle =\",angle)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show results\n",
    "f,ax =  plt.subplots(2, 2, figsize=(24,12))\n",
    "f.suptitle('Rover-centric coordinates', fontsize=28)\n",
    "\n",
    "ax[0,0].set_title('Sample Image', fontsize=24)\n",
    "ax[0,0].imshow(navi_img)\n",
    "\n",
    "ax[0,1].set_title('Warped', fontsize=24)\n",
    "ax[0,1].imshow(navi_w)\n",
    "\n",
    "ax[1,0].set_title('Warped and Thresholded', fontsize=24)\n",
    "ax[1,0].imshow(navi_t, cmap='gray')\n",
    "\n",
    "ax[1,1].set_title('Rover-centric with angle', fontsize=24)\n",
    "ax[1,1].plot(xpix, ypix, '.')\n",
    "\n",
    "ax[1,1].set_xlim([0, 160])\n",
    "ax[1,1].set_ylim([-160, 160])\n",
    "\n",
    "arrow_length = 100\n",
    "x_arrow = arrow_length * np.cos(mean_dir)\n",
    "y_arrow = arrow_length * np.sin(mean_dir)\n",
    "\n",
    "ax[1,1].arrow(0, 0, x_arrow, y_arrow, color='red', zorder=2, head_width=10, width=1)\n",
    "\n",
    "# Save figure for project report\n",
    "f.savefig('../output/rover_coords.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Test Transformations Functions: rotate_pix(), translate_pix(), and pix_to_world()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rover yaw values will come as floats from 0 to 360\n",
    "# Generate a random value in this range\n",
    "# Note: you need to convert this to radians before adding to pixel_angles\n",
    "rover_yaw = np.random.random(1)*360\n",
    "\n",
    "# Generate a random rover position in world coords\n",
    "# Position values will range from 20 to 180 to avoid the edges in a 200 x 200 pixel world\n",
    "rover_xpos = np.random.random(1)*160 + 20\n",
    "rover_ypos = np.random.random(1)*160 + 20\n",
    "\n",
    "worldmap = np.zeros((200, 200))  # create empty image for use in testing\n",
    "scale = 10                       # world map scale\n",
    "\n",
    "# Note: Since we've chosen random numbers for yaw and position, \n",
    "# multiple run of the code will result in different outputs each time.\n",
    "\n",
    "# Get navigable pixel positions in world coords\n",
    "\n",
    "x_world, y_world = pix_to_world(xpix, ypix, \n",
    "                                rover_xpos, rover_ypos, rover_yaw, \n",
    "                                worldmap.shape[0], \n",
    "                                scale)\n",
    "\n",
    "\n",
    "# Add pixel positions to worldmap\n",
    "\n",
    "worldmap[y_world, x_world] += 1\n",
    "\n",
    "# Show results\n",
    "\n",
    "print('Xpos =', rover_xpos, 'Ypos =', rover_ypos, 'Yaw =', rover_yaw)\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "f.suptitle('World Map Coordinates', fontsize=28)\n",
    "\n",
    "#f.tight_layout()\n",
    "\n",
    "ax1.plot(xpix, ypix, '.')\n",
    "ax1.set_title('Rover Space', fontsize=24)\n",
    "ax1.set_ylim(-160, 160)\n",
    "ax1.set_xlim(0, 160)\n",
    "\n",
    "ax2.imshow(worldmap, cmap='gray')\n",
    "ax2.set_title('World Space', fontsize=24)\n",
    "ax2.set_ylim(0, 200)\n",
    "ax2.set_xlim(0, 200)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure for project report\n",
    "f.savefig('../output/world_space.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <span style=\"color:blue\"> Create Data Container Class</span><a id='7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Read CSV file into a dataframe\n",
    "\n",
    "CSV file generated from the Rover Simulator Recorder will be having the following data columns:\n",
    "\n",
    "(Path, SteerAngle, Throttle, Brake, Speed, X_Position, Y_Position, Pitch, Yaw, Roll)\n",
    "\n",
    "Number of rows will depend on how long the recording was done. for each row there will be on captured image in /IMG subfolder.\n",
    "\n",
    "You can pick one of the following datasets:\n",
    "#### rec1_dataset\n",
    "#### rec2_dataset\n",
    "#### test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "csv_path = '../rec2_dataset/robot_log.csv'            # path to CSV file\n",
    "\n",
    "df = pd.read_csv(csv_path,delimiter=';',decimal='.')  # read all columns/rows into dataframe\n",
    "\n",
    "print(df[0:3])                                        # show 1st 3 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Create a list of images pathnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "csv_img_list = df[\"Path\"].tolist()  # Create list of image pathnames\n",
    "\n",
    "print(csv_img_list[0])              # Show 1st sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Read Ground Truth Map and create a 3-Channel image from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "map_path = '../calibration_images/map_bw.png'     # path to ground_truth image file\n",
    "\n",
    "ground_truth = mpimg.imread(map_path)             # read ground_truth image\n",
    "\n",
    "ground_truth_3d = np.dstack((ground_truth,      # Blue channel\n",
    "                             ground_truth*255,  # Green channel\n",
    "                             ground_truth       # Red channel\n",
    "                            )\n",
    "                           ).astype(np.float)   # all BGR numbers are float\n",
    "\n",
    "# show map image\n",
    "\n",
    "f,ax =  plt.subplots(1, 1, figsize=(6,6))\n",
    "\n",
    "ax.set_title('Ground Truth Map', fontsize=24)\n",
    "ax.set_ylim(0,200)\n",
    "ax.set_xlim(0,200)\n",
    "ax.imshow(ground_truth_3d.astype(np.uint8))\n",
    "\n",
    "# Save figure for project report\n",
    "f.savefig('../output/ground_truth.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Create Data Container Class\n",
    "\n",
    " Will create a data container class and populate it with saved data from csv file and Woldmap. Worldmap is instantiated as 200 x 200 grid corresponding to a 200m x 200m space (same size as the ground truth map: 200 x 200 pixels). This encompasses the full range of output position values in x and y from the sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# define the class\n",
    "\n",
    "class Databucket():\n",
    "    def __init__(self):\n",
    "        self.images = csv_img_list  \n",
    "        self.xpos = df[\"X_Position\"].values\n",
    "        self.ypos = df[\"Y_Position\"].values\n",
    "        self.yaw  = df[\"Yaw\"].values\n",
    "        self.pitch= df[\"Pitch\"].values\n",
    "        self.roll = df[\"Roll\"].values\n",
    "        self.sangl= df[\"SteerAngle\"].values\n",
    "        self.brake= df[\"Brake\"].values\n",
    "        self.throt= df[\"Throttle\"].values\n",
    "        self.speed= df[\"Speed\"].values\n",
    "        self.count = 0 # This will be a running index\n",
    "        self.worldmap = np.zeros((200, 200, 3)).astype(np.float)\n",
    "        self.ground_truth = ground_truth_3d # Ground truth worldmap\n",
    "\n",
    "        \n",
    "# Instantiate a Databucket()\n",
    "\n",
    "data = Databucket()\n",
    "\n",
    "# this will be a global variable/object that you can refer to in the process_image() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. <span style=\"color:blue\"> Process_Image Function </span><a id='8'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to pass stored images to reading rover position and yaw angle from csv file\n",
    "# This function will be used by moviepy to create an output video\n",
    "\n",
    "def process_image(img):\n",
    "\n",
    "# 1) Define source and destination points for perspective transform\n",
    "#------------------------------------------------------------------\n",
    "    \n",
    "    scale    = 10             # each 10x10 pixel square represents 1 square meter\n",
    "    hs_size  = scale/2        # half the size of one square\n",
    "    b_ofst   = 6              # bottom offset to account for distance from rover edge to 1st camera visiable point\n",
    "    x_cntr   = img.shape[1]/2 # center of the image x axis\n",
    "    x_max    = img.shape[1]   # end of x or right edge of image\n",
    "    y_max    = img.shape[0]   # end of y or bottom edge of image\n",
    "    \n",
    "    rvr_xpos = data.xpos[data.count]\n",
    "    rvr_ypos = data.ypos[data.count]\n",
    "    rvr_yaw  = data.yaw[data.count]\n",
    "    wrl_shp0 = data.worldmap.shape[0]\n",
    "    \n",
    "    \n",
    "    src = np.float32([\n",
    "                     [14, 140],      # Left Bottom\n",
    "                     [301 ,140],     # Right Bottom\n",
    "                     [200, 96],      # Right Top\n",
    "                     [118, 96]       # Left Top\n",
    "                     ])              # above data captured from grid calibration image.\n",
    "\n",
    "    dst = np.float32([\n",
    "                     [x_cntr - hs_size, y_max - b_ofst],             # Left Bottom\n",
    "                     [x_cntr + hs_size, y_max - b_ofst],             # Right Bottom\n",
    "                     [x_cntr + hs_size, y_max - 2*hs_size - b_ofst], # Right Top\n",
    "                     [x_cntr - hs_size, y_max - 2*hs_size - b_ofst], # Left Top\n",
    "                     ])\n",
    "    \n",
    "# 2) Apply perspective transform\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "    img_w = perspect_transform(img, src, dst) #warp the image\n",
    "\n",
    "# 3) Apply color threshold to identify navigable terrain/obstacles/rock samples\n",
    "#------------------------------------------------------------------\n",
    "    navi_wt,obst_wt = navi_thresh(img_w)  # Threshold warped image to show both navigable and obstcales areas\n",
    "    rock_wt = rock_thresh(img_w)           # Threshold calibration image to isolate the rock\n",
    "    \n",
    "# 4) Convert thresholded image pixel values to rover-centric coords\n",
    "#------------------------------------------------------------------\n",
    "    navix_pix, naviy_pix = rover_coords(navi_wt)  # convert navigable area thresholded to rover coords.\n",
    "    obstx_pix, obsty_pix = rover_coords(obst_wt)  # convert obstacle area thresholded to rover coords.\n",
    "    rockx_pix, rocky_pix = rover_coords(rock_wt)  # convert rock thresholded to rover coords.\n",
    "        \n",
    "# 5) Convert rover-centric pixel values to world coords\n",
    "#------------------------------------------------------------------\n",
    "    \n",
    "    naviy_wld, navix_wld = pix_to_world(navix_pix, naviy_pix,          # convert navigable area\n",
    "                                        rvr_xpos, rvr_ypos, rvr_yaw,\n",
    "                                        wrl_shp0,scale)\n",
    "    \n",
    "    obsty_wld, obstx_wld = pix_to_world(obstx_pix, obsty_pix,          # convert obsticale area\n",
    "                                        rvr_xpos, rvr_ypos, rvr_yaw,\n",
    "                                        wrl_shp0,scale)\n",
    "    \n",
    "    rocky_wld, rockx_wld = pix_to_world(rockx_pix, rocky_pix,          # convert rock area\n",
    "                                        rvr_xpos, rvr_ypos, rvr_yaw,\n",
    "                                        wrl_shp0,scale)\n",
    "    \n",
    "    \n",
    "    \n",
    "# 6) Update worldmap (to be displayed on right side of screen)\n",
    "#------------------------------------------------------------------\n",
    "    data.worldmap[obstx_wld, obsty_wld, 0] = 255 # set obsticale area pixels to RED\n",
    "    data.worldmap[navix_wld, naviy_wld, 2] = 255 # set navigable area pixels to BLUE\n",
    "    data.worldmap[rockx_wld, rocky_wld, :] = 255 # set rock area to WHITE\n",
    "\n",
    "    navi = data.worldmap[:,:,2]==255 # index of all navigable pixels\n",
    "    data.worldmap[navi,0]=0          # reset corresponding obsticale pixels\n",
    "    \n",
    "    \n",
    "# 7) Make a mosaic image\n",
    "#------------------------------------------------------------------     \n",
    "    frame = np.zeros((img.shape[0] + data.worldmap.shape[0], \n",
    "                      img.shape[1]*2, \n",
    "                      3))                          # create a blank image\n",
    "    frame[0:img.shape[0], 0:img.shape[1]] = img    # add camera image to top-left\n",
    "    frame[0:img.shape[0], img.shape[1]:]  = img_w  # add warped image to top-right\n",
    "\n",
    "    map_add = cv2.addWeighted(data.worldmap,1,     # mix world map with ground truth\n",
    "                              data.ground_truth,0.5,\n",
    "                              0) \n",
    "\n",
    "    \n",
    "# 8) Add rover location and direction markers to mosaic image\n",
    "#------------------------------------------------------------------    \n",
    "\n",
    "    \n",
    "    ryaw  = rvr_yaw*np.pi/180               # convert yaw from deg to rad\n",
    "    \n",
    "    ac = (0,0,255)                          # arrow color\n",
    "    al = 5                                  # arrow half length\n",
    "    fx = int(rvr_xpos+(al * np.cos(ryaw)))  # arrow head point x based on current rover yaw\n",
    "    fy = int(rvr_ypos+(al * np.sin(ryaw)))  # arrow head point y based on current rover yaw\n",
    "    bx = int(rvr_xpos)                      # arrow start point x\n",
    "    by = int(rvr_ypos)                      # arrow start point y\n",
    "            \n",
    "    cv2.arrowedLine(map_add,(bx,by),(fx,fy),ac)\n",
    "        \n",
    "    frame[img.shape[0]:, 0:data.worldmap.shape[1]] = np.flipud(map_add)  # add mixed map to bottom-left\n",
    "\n",
    "\n",
    "# 9) Show telemetry values (bottom-right)\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "    # Calculate some statistics on the map results\n",
    "    \n",
    "    # total pix in the navigable area\n",
    "    tot_nav_pix = np.float(len((data.worldmap[:,:,2].nonzero()[0])))  \n",
    "    \n",
    "    # how many correspond to truth\n",
    "    good_nav_pix = np.float(len(((data.worldmap[:,:,2] > 0) & (data.ground_truth[:,:,1] > 0)).nonzero()[0]))\n",
    "    \n",
    "    # how many do not correspond to truth\n",
    "    bad_nav_pix = np.float(len(((data.worldmap[:,:,2] > 0) & (data.ground_truth[:,:,1] == 0)).nonzero()[0]))\n",
    "    \n",
    "    # Grab the total number of map pixels\n",
    "    tot_map_pix = np.float(len((data.ground_truth[:,:,1].nonzero()[0])))\n",
    "    \n",
    "    # Calculate the percentage of ground truth map that has been successfully found\n",
    "    perc_mapped = round(100*good_nav_pix/tot_map_pix, 1)\n",
    "    \n",
    "    # number of good map pixel detections divided by total pixels found to be navigable terrain\n",
    "    if tot_nav_pix > 0:\n",
    "        fidelity = round(100*good_nav_pix/(tot_nav_pix), 1)\n",
    "    else:\n",
    "        fidelity = 0\n",
    "\n",
    "\n",
    "\n",
    "    # text style\n",
    "    tox =img.shape[1]+20        # Text X offset from top left corner\n",
    "    toy =img.shape[0]+20        # Text Y offset from top left corner\n",
    "    tc  =(255, 255, 255)          # Text color\n",
    "    tf  =cv2.FONT_HERSHEY_COMPLEX # Text font\n",
    "    ts  =0.4                      # Text size\n",
    "    \n",
    "    # text value\n",
    "    txt1=\"Xpos=\"+str(np.round(data.xpos[data.count],2))+\\\n",
    "         \" Ypos=\"+str(np.round(data.ypos[data.count],2))\n",
    "        \n",
    "    txt2=\"Yaw=\"+str(np.round(data.yaw[data.count],2))+\\\n",
    "         \" Roll=\"+str(np.round(data.roll[data.count],2))\n",
    "            \n",
    "    txt3=\"Pitch=\"+str(np.round(data.pitch[data.count],2))+\\\n",
    "         \" S.Angle=\"+str(np.round(data.sangl[data.count],2))\n",
    "         \n",
    "    \n",
    "    txt4=\"Throttle=\"+str(np.round(data.throt[data.count],2))+\\\n",
    "         \" Speed=\"+str(np.round(data.speed[data.count],2))\n",
    "         \n",
    "    txt5=\"Brake=\"+str(np.round(data.brake[data.count],2))\n",
    "    \n",
    "    txt6=\"Mapped: \"+str(perc_mapped)+'%'\n",
    "    \n",
    "    txt7=\"Fidelity: \"+str(fidelity)+'%'\n",
    "    \n",
    "    # Add text to frame\n",
    "    cv2.putText(frame,txt1,(tox,toy+20),tf,ts,tc,1)\n",
    "    cv2.putText(frame,txt2,(tox,toy+40),tf,ts,tc,1)\n",
    "    cv2.putText(frame,txt3,(tox,toy+60),tf,ts,tc,1)\n",
    "    cv2.putText(frame,txt4,(tox,toy+80),tf,ts,tc,1)\n",
    "    cv2.putText(frame,txt5,(tox,toy+100),tf,ts,tc,1)\n",
    "    cv2.putText(frame,txt6,(tox,toy+120),tf,ts,tc,1)\n",
    "    cv2.putText(frame,txt7,(tox,toy+140),tf,ts,tc,1)\n",
    "#------------------------------------------------------------------\n",
    "    \n",
    "    if data.count < len(data.images) - 1:\n",
    "        data.count += 1 # Keep track of the index in the Databucket()\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function with one sample\n",
    "data = Databucket()\n",
    "\n",
    "# Grab random image from data\n",
    "idx = np.random.randint(0, len(data.images) - 1) \n",
    "navi_img = mpimg.imread(data.images[idx])\n",
    "tst_frame = process_image(navi_img)\n",
    "\n",
    "# Show frame\n",
    "f,ax =  plt.subplots(1, 1, figsize=(14,14))\n",
    "ax.set_title('One Sample Video Frame', fontsize=20)\n",
    "ax.imshow(tst_frame.astype(np.uint8))\n",
    "\n",
    "# Save figure for project report\n",
    "f.savefig('../output/test_frame.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. <span style=\"color:blue\"> Make a video from processed image data </span><a id='9'></a>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "\n",
    "# Define pathname to save the output video\n",
    "output = '../output/test_mapping.mp4'\n",
    "data = Databucket() # Re-initialize data in case you're running this cell multiple times\n",
    "clip = ImageSequenceClip(data.images, fps=50) # Note: output video will be sped up because \n",
    "                                          # recording rate in simulator is fps=25\n",
    "new_clip = clip.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time new_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## 10. <span style=\"color:blue\"> Play Video </span><a id='10'></a>\n",
    "### 10.1. Video Player using HTML\n",
    "\n",
    "This next cell should function as an inline video player\n",
    "If this fails to render the video, try running the following cell (alternative video rendering method). You can also simply have a look at the saved mp4 in your /output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '../output/test_mapping.mp4'\n",
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2. Alternative video rendering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "video = io.open(output, 'r+b').read()\n",
    "encoded_video = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded_video.decode('ascii')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
